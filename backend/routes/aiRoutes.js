const express = require('express');
const { exec } = require('child_process');
const { promisify } = require('util');
const fs = require('fs-extra');
const path = require('path');
const pdf = require('pdf-parse');
const Groq = require('groq-sdk');

const router = express.Router();
const execAsync = promisify(exec);

// Initialize Groq client (will be set with API key from settings)
let groqClient = null;

const initGroqClient = (apiKey) => {
  if (apiKey) {
    groqClient = new Groq({ apiKey });
    return true;
  }
  return false;
};

// V√©rifier le statut d'Ollama
router.get('/ollama/status', async (req, res) => {
  try {
    // V√©rifier si Ollama est install√© et en cours d'ex√©cution
    const response = await fetch('http://localhost:11434/api/tags');
    const data = await response.json();

    res.json({
      success: true,
      data: {
        installed: true,
        running: true,
        models: data.models || []
      }
    });
  } catch (error) {
    // V√©rifier si Ollama est install√© mais pas en cours d'ex√©cution
    try {
      await execAsync('ollama --version');
      res.json({
        success: true,
        data: {
          installed: true,
          running: false,
          models: []
        }
      });
    } catch (versionError) {
      res.json({
        success: true,
        data: {
          installed: false,
          running: false,
          models: []
        }
      });
    }
  }
});

// Obtenir la liste des mod√®les disponibles
router.get('/ollama/models/available', async (req, res) => {
  try {
    const availableModels = [
      {
        name: 'llama3.2:3b',
        size: '2.0GB',
        description: 'Mod√®le l√©ger, excellent pour commencer',
        recommended: true
      },
      {
        name: 'phi3:mini',
        size: '2.3GB',
        description: 'Tr√®s efficace pour Q&A, optimis√© Microsoft',
        recommended: true
      },
      {
        name: 'mistral:7b',
        size: '4.1GB',
        description: 'Mod√®le plus puissant, n√©cessite plus de VRAM',
        recommended: false
      },
      {
        name: 'llama3.2:1b',
        size: '1.3GB',
        description: 'Ultra-l√©ger, pour machines tr√®s limit√©es',
        recommended: false
      },
      {
        name: 'qwen2.5:7b',
        size: '4.7GB',
        description: 'Excellent pour l\'analyse de documents',
        recommended: false
      }
    ];

    res.json({
      success: true,
      data: availableModels
    });
  } catch (error) {
    console.error('Error getting available models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des mod√®les disponibles'
    });
  }
});

// Rechercher dans la biblioth√®que compl√®te des mod√®les Ollama
router.get('/ollama/models/search', async (req, res) => {
  try {
    const { q: query } = req.query;

    // Base de donn√©es √©tendue des mod√®les populaires Ollama
    const fullModelLibrary = [
      // LLaMA Family
      { name: 'llama3.2:1b', size: '1.3GB', description: 'Ultra-l√©ger, pour machines tr√®s limit√©es', category: 'LLaMA', params: '1B' },
      { name: 'llama3.2:3b', size: '2.0GB', description: 'Mod√®le l√©ger, excellent pour commencer', category: 'LLaMA', params: '3B' },
      { name: 'llama3.1:8b', size: '4.7GB', description: 'Mod√®le √©quilibr√© performance/taille', category: 'LLaMA', params: '8B' },
      { name: 'llama3.1:70b', size: '40GB', description: 'Mod√®le tr√®s puissant, n√©cessite beaucoup de VRAM', category: 'LLaMA', params: '70B' },

      // Mistral Family
      { name: 'mistral:7b', size: '4.1GB', description: 'Mod√®le puissant, excellent pour le fran√ßais', category: 'Mistral', params: '7B' },
      { name: 'mistral-nemo:12b', size: '7.1GB', description: 'Version am√©lior√©e de Mistral', category: 'Mistral', params: '12B' },
      { name: 'mixtral:8x7b', size: '26GB', description: 'Mod√®le mixture of experts tr√®s performant', category: 'Mistral', params: '47B' },

      // Phi Family (Microsoft)
      { name: 'phi3:mini', size: '2.3GB', description: 'Tr√®s efficace pour Q&A, optimis√© Microsoft', category: 'Phi', params: '3.8B' },
      { name: 'phi3:medium', size: '7.9GB', description: 'Version plus puissante de Phi-3', category: 'Phi', params: '14B' },

      // Qwen Family (Alibaba)
      { name: 'qwen2.5:0.5b', size: '0.4GB', description: 'Ultra-compact, id√©al pour tests', category: 'Qwen', params: '0.5B' },
      { name: 'qwen2.5:1.5b', size: '0.9GB', description: 'Tr√®s l√©ger mais performant', category: 'Qwen', params: '1.5B' },
      { name: 'qwen2.5:3b', size: '1.9GB', description: 'Bon √©quilibre taille/performance', category: 'Qwen', params: '3B' },
      { name: 'qwen2.5:7b', size: '4.7GB', description: 'Excellent pour l\'analyse de documents', category: 'Qwen', params: '7B' },
      { name: 'qwen2.5:14b', size: '8.2GB', description: 'Version plus puissante', category: 'Qwen', params: '14B' },
      { name: 'qwen2.5:32b', size: '18GB', description: 'Tr√®s performant pour les t√¢ches complexes', category: 'Qwen', params: '32B' },

      // Code Models
      { name: 'codellama:7b', size: '3.8GB', description: 'Sp√©cialis√© dans la g√©n√©ration de code', category: 'Code', params: '7B' },
      { name: 'codegemma:7b', size: '5.0GB', description: 'Mod√®le Google sp√©cialis√© code', category: 'Code', params: '7B' },
      { name: 'deepseek-coder:6.7b', size: '3.8GB', description: 'Excellent pour le code, tr√®s performant', category: 'Code', params: '6.7B' },
      { name: 'starcoder2:3b', size: '1.7GB', description: 'Mod√®le de code l√©ger et efficace', category: 'Code', params: '3B' },

      // Specialized Models
      { name: 'gemma2:2b', size: '1.6GB', description: 'Mod√®le Google compact et efficace', category: 'Gemma', params: '2B' },
      { name: 'gemma2:9b', size: '5.4GB', description: 'Version plus puissante de Gemma', category: 'Gemma', params: '9B' },
      { name: 'gemma2:27b', size: '16GB', description: 'Mod√®le Google haut de gamme', category: 'Gemma', params: '27B' },

      // Embedding Models
      { name: 'nomic-embed-text', size: '274MB', description: 'Mod√®le d\'embeddings pour la recherche s√©mantique', category: 'Embedding', params: '137M' },
      { name: 'mxbai-embed-large', size: '669MB', description: 'Embeddings haute qualit√©', category: 'Embedding', params: '335M' },

      // Vision Models
      { name: 'llava:7b', size: '4.7GB', description: 'Mod√®le multimodal (texte + images)', category: 'Vision', params: '7B' },
      { name: 'llava:13b', size: '8.0GB', description: 'Version plus puissante de LLaVA', category: 'Vision', params: '13B' },

      // Other Popular Models
      { name: 'neural-chat:7b', size: '4.1GB', description: 'Optimis√© pour les conversations', category: 'Chat', params: '7B' },
      { name: 'openchat:7b', size: '4.1GB', description: 'Mod√®le de chat tr√®s performant', category: 'Chat', params: '7B' },
      { name: 'wizard-vicuna-uncensored:7b', size: '4.1GB', description: 'Mod√®le sans censure pour cr√©ativit√©', category: 'Uncensored', params: '7B' },
      { name: 'orca2:7b', size: '4.1GB', description: 'Mod√®le Microsoft pour raisonnement', category: 'Reasoning', params: '7B' },
      { name: 'dolphin-mixtral:8x7b', size: '26GB', description: 'Version fine-tun√©e de Mixtral', category: 'Dolphin', params: '47B' },
    ];

    let results = fullModelLibrary;

    // Filtrer par query si fournie
    if (query && query.trim()) {
      const searchTerm = query.toLowerCase();
      results = fullModelLibrary.filter(model =>
        model.name.toLowerCase().includes(searchTerm) ||
        model.description.toLowerCase().includes(searchTerm) ||
        model.category.toLowerCase().includes(searchTerm)
      );
    }

    // Trier par popularit√©/recommandation (les plus petits d'abord dans chaque cat√©gorie)
    results.sort((a, b) => {
      // D'abord par cat√©gorie
      if (a.category !== b.category) {
        const categoryOrder = ['LLaMA', 'Mistral', 'Phi', 'Qwen', 'Gemma', 'Code', 'Chat', 'Vision', 'Embedding'];
        return categoryOrder.indexOf(a.category) - categoryOrder.indexOf(b.category);
      }
      // Puis par taille (plus petit d'abord)
      const sizeA = parseFloat(a.size);
      const sizeB = parseFloat(b.size);
      return sizeA - sizeB;
    });

    res.json({
      success: true,
      data: {
        models: results,
        total: results.length,
        query: query || ''
      }
    });
  } catch (error) {
    console.error('Error searching models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la recherche de mod√®les'
    });
  }
});

// Obtenir la liste des mod√®les install√©s
router.get('/ollama/models/installed', async (req, res) => {
  try {
    const response = await fetch('http://localhost:11434/api/tags');
    const data = await response.json();

    res.json({
      success: true,
      data: data.models || []
    });
  } catch (error) {
    console.error('Error getting installed models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des mod√®les install√©s'
    });
  }
});

// T√©l√©charger un mod√®le
router.post('/ollama/models/pull', async (req, res) => {
  const { modelName } = req.body;

  if (!modelName) {
    return res.status(400).json({
      success: false,
      error: 'Le nom du mod√®le est requis'
    });
  }

  try {
    // D√©marrer le t√©l√©chargement en arri√®re-plan avec logging am√©lior√©
    console.log(`üöÄ D√©marrage du t√©l√©chargement de ${modelName}...`);

    const pullProcess = exec(`ollama pull ${modelName}`, (error, stdout, stderr) => {
      if (error) {
        console.error(`‚ùå Erreur lors du t√©l√©chargement de ${modelName}:`, error);
        return;
      }
      console.log(`‚úÖ Mod√®le ${modelName} t√©l√©charg√© avec succ√®s`);
      console.log('Sortie:', stdout);
    });

    // Log progress en temps r√©el
    pullProcess.stdout?.on('data', (data) => {
      console.log(`üì• ${modelName}: ${data.toString().trim()}`);
    });

    pullProcess.stderr?.on('data', (data) => {
      console.log(`üîÑ ${modelName}: ${data.toString().trim()}`);
    });

    res.json({
      success: true,
      message: `T√©l√©chargement du mod√®le ${modelName} d√©marr√©. V√©rifiez les logs du serveur pour le progr√®s.`,
      data: {
        modelName,
        tip: 'Le t√©l√©chargement peut prendre plusieurs minutes selon la taille du mod√®le. Utilisez le bouton "Actualiser" pour v√©rifier l\'installation.'
      }
    });
  } catch (error) {
    console.error('Error starting model pull:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors du d√©marrage du t√©l√©chargement'
    });
  }
});

// Supprimer un mod√®le
router.delete('/ollama/models/:modelName', async (req, res) => {
  const { modelName } = req.params;

  try {
    await execAsync(`ollama rm ${modelName}`);

    res.json({
      success: true,
      message: `Mod√®le ${modelName} supprim√© avec succ√®s`
    });
  } catch (error) {
    console.error(`Error removing model ${modelName}:`, error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la suppression du mod√®le'
    });
  }
});

// Tester la connexion avec un mod√®le
router.post('/ollama/test', async (req, res) => {
  const { modelName } = req.body;

  if (!modelName) {
    return res.status(400).json({
      success: false,
      error: 'Le nom du mod√®le est requis'
    });
  }

  try {
    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: modelName,
        prompt: 'R√©ponds simplement "Test r√©ussi" en fran√ßais.',
        stream: false
      })
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();

    res.json({
      success: true,
      message: 'Test de connexion r√©ussi',
      data: {
        modelName,
        response: data.response
      }
    });
  } catch (error) {
    console.error(`Error testing model ${modelName}:`, error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors du test de connexion'
    });
  }
});

// Obtenir les informations syst√®me (VRAM, etc.)
router.get('/system/info', async (req, res) => {
  try {
    // Utiliser les commandes syst√®me pour obtenir des infos sur la VRAM/RAM
    let systemInfo = {
      ram: 'N/A',
      gpu: 'N/A',
      storage: 'N/A'
    };

    try {
      // Tentative d'obtenir des infos RAM (Windows)
      const { stdout: ramInfo } = await execAsync('wmic computersystem get TotalPhysicalMemory /value');
      const ramMatch = ramInfo.match(/TotalPhysicalMemory=(\d+)/);
      if (ramMatch) {
        const ramBytes = parseInt(ramMatch[1]);
        const ramGB = Math.round(ramBytes / (1024 * 1024 * 1024));
        systemInfo.ram = `${ramGB} GB`;
      }
    } catch (error) {
      console.log('Could not get RAM info:', error.message);
    }

    try {
      // Tentative d'obtenir des infos GPU (Windows)
      const { stdout: gpuInfo } = await execAsync('wmic path win32_VideoController get name /value');
      const gpuMatch = gpuInfo.match(/Name=(.+)/);
      if (gpuMatch) {
        systemInfo.gpu = gpuMatch[1].trim();
      }
    } catch (error) {
      console.log('Could not get GPU info:', error.message);
    }

    res.json({
      success: true,
      data: systemInfo
    });
  } catch (error) {
    console.error('Error getting system info:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des informations syst√®me'
    });
  }
});

// Extraire le texte d'un PDF pour l'analyse IA
router.post('/pdf/extract', async (req, res) => {
  try {
    const { paperId } = req.body;

    if (!paperId) {
      return res.status(400).json({
        success: false,
        error: 'L\'ID du paper est requis'
      });
    }

    // Trouver le fichier PDF dans le dossier MyPapers
    const paperFolderPath = path.join(__dirname, '..', 'MyPapers');
    const folders = await fs.readdir(paperFolderPath);

    let pdfPath = null;
    let foundFolder = null;

    // Chercher le dossier contenant l'ID du paper
    for (const folder of folders) {
      if (folder.includes(`_${paperId}`)) {
        foundFolder = folder;
        const folderPath = path.join(paperFolderPath, folder);
        const files = await fs.readdir(folderPath);

        // Chercher le fichier PDF
        const pdfFile = files.find(file => file.toLowerCase().endsWith('.pdf'));
        if (pdfFile) {
          pdfPath = path.join(folderPath, pdfFile);
          break;
        }
      }
    }

    if (!pdfPath || !await fs.pathExists(pdfPath)) {
      return res.status(404).json({
        success: false,
        error: 'Fichier PDF non trouv√© pour cet article'
      });
    }

    // Extraire le texte du PDF
    const dataBuffer = await fs.readFile(pdfPath);
    const data = await pdf(dataBuffer);

    res.json({
      success: true,
      data: {
        text: data.text,
        pages: data.numpages,
        info: data.info,
        metadata: {
          paperId,
          folderPath: foundFolder,
          pdfPath: path.basename(pdfPath),
          extractedAt: new Date().toISOString()
        }
      }
    });

  } catch (error) {
    console.error('Error extracting PDF text:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de l\'extraction du texte PDF'
    });
  }
});

// Get available Groq models
router.get('/groq/models', async (req, res) => {
  try {
    const groqModels = [
      // Llama 3.3 (Latest)
      {
        name: 'llama-3.3-70b-versatile',
        displayName: 'Llama 3.3 70B Versatile',
        description: 'Le plus r√©cent et puissant, excellent pour l\'analyse d√©taill√©e',
        contextWindow: 128000,
        recommended: true
      },
      {
        name: 'llama-3.3-70b-specdec',
        displayName: 'Llama 3.3 70B Speculative Decoding',
        description: 'Version optimis√©e pour la vitesse avec d√©codage sp√©culatif',
        contextWindow: 8192,
        recommended: false
      },

      // Llama 3.1
      {
        name: 'llama-3.1-8b-instant',
        displayName: 'Llama 3.1 8B Instant',
        description: 'Ultra-rapide, bon √©quilibre performance/vitesse',
        contextWindow: 128000,
        recommended: true
      },
      {
        name: 'llama-3.1-70b-versatile',
        displayName: 'Llama 3.1 70B Versatile',
        description: 'Tr√®s puissant, polyvalent pour toutes t√¢ches',
        contextWindow: 128000,
        recommended: false
      },

      // Llama 3.2 (Vision capable)
      {
        name: 'llama-3.2-1b-preview',
        displayName: 'Llama 3.2 1B Preview',
        description: 'Mod√®le ultra-l√©ger, tr√®s rapide',
        contextWindow: 128000,
        recommended: false
      },
      {
        name: 'llama-3.2-3b-preview',
        displayName: 'Llama 3.2 3B Preview',
        description: 'L√©ger et efficace',
        contextWindow: 128000,
        recommended: false
      },
      {
        name: 'llama-3.2-11b-vision-preview',
        displayName: 'Llama 3.2 11B Vision',
        description: 'Capable d\'analyser des images (multimodal)',
        contextWindow: 128000,
        recommended: false
      },
      {
        name: 'llama-3.2-90b-vision-preview',
        displayName: 'Llama 3.2 90B Vision',
        description: 'Tr√®s puissant avec capacit√© vision (multimodal)',
        contextWindow: 128000,
        recommended: false
      },

      // Llama 3 (Original)
      {
        name: 'llama3-8b-8192',
        displayName: 'Llama 3 8B',
        description: 'Rapide et efficace',
        contextWindow: 8192,
        recommended: false
      },
      {
        name: 'llama3-70b-8192',
        displayName: 'Llama 3 70B',
        description: 'Puissant et rapide',
        contextWindow: 8192,
        recommended: false
      },
      {
        name: 'llama-guard-3-8b',
        displayName: 'Llama Guard 3 8B',
        description: 'Mod√®le de mod√©ration et s√©curit√©',
        contextWindow: 8192,
        recommended: false
      },

      // Mixtral (Mistral AI)
      {
        name: 'mixtral-8x7b-32768',
        displayName: 'Mixtral 8x7B',
        description: 'Excellent pour le raisonnement complexe',
        contextWindow: 32768,
        recommended: false
      },

      // Gemma (Google)
      {
        name: 'gemma2-9b-it',
        displayName: 'Gemma 2 9B',
        description: 'Optimis√© par Google, tr√®s pr√©cis',
        contextWindow: 8192,
        recommended: false
      },
      {
        name: 'gemma-7b-it',
        displayName: 'Gemma 7B',
        description: 'L√©ger et efficace, par Google',
        contextWindow: 8192,
        recommended: false
      },

      // Llama 3 Groq Tool Use (optimis√© pour function calling)
      {
        name: 'llama3-groq-8b-8192-tool-use-preview',
        displayName: 'Llama 3 Groq 8B Tool Use',
        description: 'Optimis√© pour l\'utilisation d\'outils et function calling',
        contextWindow: 8192,
        recommended: false
      },
      {
        name: 'llama3-groq-70b-8192-tool-use-preview',
        displayName: 'Llama 3 Groq 70B Tool Use',
        description: 'Version puissante optimis√©e pour l\'utilisation d\'outils',
        contextWindow: 8192,
        recommended: false
      },

      // DeepSeek
      {
        name: 'deepseek-r1-distill-llama-70b',
        displayName: 'DeepSeek R1 Distill Llama 70B',
        description: 'Mod√®le DeepSeek distill√©, excellent raisonnement',
        contextWindow: 128000,
        recommended: false
      },

      // Llama 4 Maverick (Experimental)
      {
        name: 'llama-4-maverick-405b',
        displayName: 'Llama 4 Maverick 405B',
        description: 'Mod√®le exp√©rimental tr√®s puissant (preview)',
        contextWindow: 128000,
        recommended: false
      },

      // OpenAI GPT OSS
      {
        name: 'openai/gpt-oss-120b',
        displayName: 'OpenAI GPT OSS 120B',
        description: 'Version open-source exp√©rimentale d\'OpenAI GPT',
        contextWindow: 32768,
        recommended: false
      },

      // Qwen (Alibaba)
      {
        name: 'qwen-2.5-72b',
        displayName: 'Qwen 2.5 72B',
        description: 'Mod√®le puissant d\'Alibaba, excellent pour le code',
        contextWindow: 32768,
        recommended: false
      },

      // Distil-Whisper (pour transcription audio)
      {
        name: 'distil-whisper-large-v3-en',
        displayName: 'Distil-Whisper Large v3 English',
        description: 'Transcription audio en anglais (Speech-to-Text)',
        contextWindow: null,
        recommended: false
      },
      {
        name: 'whisper-large-v3',
        displayName: 'Whisper Large v3',
        description: 'Transcription audio multilingue (Speech-to-Text)',
        contextWindow: null,
        recommended: false
      },
      {
        name: 'whisper-large-v3-turbo',
        displayName: 'Whisper Large v3 Turbo',
        description: 'Transcription audio rapide multilingue',
        contextWindow: null,
        recommended: false
      }
    ];

    res.json({
      success: true,
      data: groqModels
    });
  } catch (error) {
    console.error('Error getting Groq models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des mod√®les Groq'
    });
  }
});

// Set Groq API key
router.post('/groq/api-key', async (req, res) => {
  try {
    const { apiKey } = req.body;

    if (!apiKey) {
      return res.status(400).json({
        success: false,
        error: 'La cl√© API est requise'
      });
    }

    // Test the API key
    const testClient = new Groq({ apiKey });
    await testClient.chat.completions.create({
      messages: [{ role: 'user', content: 'test' }],
      model: 'llama-3.1-8b-instant',
      max_tokens: 10
    });

    // If successful, save and initialize
    initGroqClient(apiKey);

    // Save to settings file
    const settingsPath = path.join(__dirname, '..', 'settings.json');
    let settings = {};

    if (await fs.pathExists(settingsPath)) {
      settings = await fs.readJson(settingsPath);
    }

    settings.groqApiKey = apiKey;
    await fs.writeJson(settingsPath, settings, { spaces: 2 });

    res.json({
      success: true,
      message: 'Cl√© API Groq enregistr√©e avec succ√®s'
    });
  } catch (error) {
    console.error('Error setting Groq API key:', error);
    res.status(500).json({
      success: false,
      error: 'Cl√© API invalide ou erreur de connexion'
    });
  }
});

// Get settings (including AI provider preference)
router.get('/settings', async (req, res) => {
  try {
    const settingsPath = path.join(__dirname, '..', 'settings.json');

    if (await fs.pathExists(settingsPath)) {
      const settings = await fs.readJson(settingsPath);

      // Don't send the full API key, just whether it exists
      res.json({
        success: true,
        data: {
          aiProvider: settings.aiProvider || 'ollama',
          hasGroqApiKey: !!settings.groqApiKey
        }
      });
    } else {
      res.json({
        success: true,
        data: {
          aiProvider: 'ollama',
          hasGroqApiKey: false
        }
      });
    }
  } catch (error) {
    console.error('Error getting settings:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des param√®tres'
    });
  }
});

// Save settings
router.post('/settings', async (req, res) => {
  try {
    const { aiProvider } = req.body;
    const settingsPath = path.join(__dirname, '..', 'settings.json');

    let settings = {};
    if (await fs.pathExists(settingsPath)) {
      settings = await fs.readJson(settingsPath);
    }

    if (aiProvider) {
      settings.aiProvider = aiProvider;
    }

    await fs.writeJson(settingsPath, settings, { spaces: 2 });

    res.json({
      success: true,
      message: 'Param√®tres enregistr√©s avec succ√®s'
    });
  } catch (error) {
    console.error('Error saving settings:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la sauvegarde des param√®tres'
    });
  }
});

// Chat avec l'IA sur un document
router.post('/chat', async (req, res) => {
  try {
    const { message, context, modelName = 'llama3.1:8b', history = [], provider = 'ollama' } = req.body;

    if (!message) {
      return res.status(400).json({
        success: false,
        error: 'Le message est requis'
      });
    }

    // Construire le prompt avec le contexte du document et l'historique
    let fullPrompt = '';

    if (context) {
      const contextText = typeof context === 'string' ? context : (context.text || '');

      if (contextText) {
        fullPrompt = `Tu es un assistant IA sp√©cialis√© dans l'analyse d'articles de recherche scientifique.

CONTEXTE - Voici le contenu de l'article √† analyser :

${contextText.substring(0, 20000)} ${contextText.length > 20000 ? '...' : ''}
`;

        // Ajouter l'historique de conversation s'il existe
        if (history && history.length > 0) {
          fullPrompt += `\nHISTORIQUE DE CONVERSATION :\n`;
          history.slice(-6).forEach(msg => { // Garder seulement les 6 derniers messages
            const role = msg.type === 'user' ? 'UTILISATEUR' : 'ASSISTANT';
            fullPrompt += `${role}: ${msg.content}\n`;
          });
        }

        fullPrompt += `
NOUVELLE QUESTION DE L'UTILISATEUR :
${message}

INSTRUCTIONS :
- R√©ponds uniquement en fran√ßais
- Base ta r√©ponse sur le contenu de l'article fourni
- Tiens compte de l'historique de conversation pour fournir des r√©ponses contextuelles
- Sois pr√©cis et factuel
- Si l'information n'est pas dans l'article, dis-le clairement
- Structure ta r√©ponse de mani√®re claire et lisible

R√âPONSE :`;
      } else {
        fullPrompt = `Tu es un assistant IA pour l'analyse d'articles scientifiques. R√©ponds en fran√ßais √† cette question : ${message}`;
      }
    } else {
      fullPrompt = `Tu es un assistant IA pour l'analyse d'articles scientifiques. R√©ponds en fran√ßais √† cette question : ${message}`;
    }

    if (provider === 'groq') {
      // Use Groq
      if (!groqClient) {
        // Try to load API key from settings
        const settingsPath = path.join(__dirname, '..', 'settings.json');
        if (await fs.pathExists(settingsPath)) {
          const settings = await fs.readJson(settingsPath);
          if (settings.groqApiKey) {
            initGroqClient(settings.groqApiKey);
          }
        }
      }

      if (!groqClient) {
        return res.status(400).json({
          success: false,
          error: 'Cl√© API Groq non configur√©e'
        });
      }

      // Build messages for Groq (chat format)
      const messages = [];

      if (context) {
        const contextText = typeof context === 'string' ? context : (context.text || '');
        if (contextText) {
          messages.push({
            role: 'system',
            content: `Tu es un assistant IA sp√©cialis√© dans l'analyse d'articles de recherche scientifique. Voici le contenu de l'article √† analyser :\n\n${contextText.substring(0, 30000)}`
          });
        }
      }

      // Add history
      if (history && history.length > 0) {
        history.slice(-6).forEach(msg => {
          messages.push({
            role: msg.type === 'user' ? 'user' : 'assistant',
            content: msg.content
          });
        });
      }

      // Add current message
      messages.push({
        role: 'user',
        content: message
      });

      const startTime = Date.now();
      const completion = await groqClient.chat.completions.create({
        model: modelName,
        messages: messages,
        temperature: 0.1,
        max_tokens: 2048,
      });

      const processingTime = Date.now() - startTime;

      res.json({
        success: true,
        data: {
          response: completion.choices[0]?.message?.content || 'Pas de r√©ponse',
          model: modelName,
          timestamp: new Date().toISOString(),
          tokenCount: completion.usage?.total_tokens || 0,
          processingTime: processingTime,
          provider: 'groq'
        }
      });

    } else {
      // Use Ollama (existing code)
      const response = await fetch('http://localhost:11434/api/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: modelName,
          prompt: fullPrompt,
          stream: false,
          options: {
            temperature: 0.1,
            top_p: 0.9,
            top_k: 40
          }
        })
      });

      if (!response.ok) {
        throw new Error(`Ollama error: ${response.status}`);
      }

      const data = await response.json();

      res.json({
        success: true,
        data: {
          response: data.response,
          model: modelName,
          timestamp: new Date().toISOString(),
          tokenCount: data.eval_count || 0,
          processingTime: data.eval_duration ? Math.round(data.eval_duration / 1000000) : 0,
          provider: 'ollama'
        }
      });
    }

  } catch (error) {
    console.error('Error in AI chat:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la communication avec l\'IA'
    });
  }
});

// Sauvegarder l'historique du chat
router.post('/chat/history/:paperId', async (req, res) => {
  try {
    const { paperId } = req.params;
    const { messages } = req.body;

    if (!paperId || !messages) {
      return res.status(400).json({
        success: false,
        error: 'L\'ID du paper et les messages sont requis'
      });
    }

    // Trouver le dossier du paper
    const paperFolderPath = path.join(__dirname, '..', 'MyPapers');
    const folders = await fs.readdir(paperFolderPath);

    let foundFolder = null;
    for (const folder of folders) {
      if (folder.includes(`_${paperId}`)) {
        foundFolder = folder;
        break;
      }
    }

    if (!foundFolder) {
      return res.status(404).json({
        success: false,
        error: 'Dossier du paper non trouv√©'
      });
    }

    const chatHistoryPath = path.join(paperFolderPath, foundFolder, `${paperId}_aiChat.json`);

    // Sauvegarder l'historique
    await fs.writeJson(chatHistoryPath, {
      paperId,
      messages,
      lastUpdated: new Date().toISOString()
    }, { spaces: 2 });

    res.json({
      success: true,
      message: 'Historique sauvegard√© avec succ√®s'
    });

  } catch (error) {
    console.error('Error saving chat history:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la sauvegarde de l\'historique'
    });
  }
});

// Charger l'historique du chat
router.get('/chat/history/:paperId', async (req, res) => {
  try {
    const { paperId } = req.params;

    // Trouver le dossier du paper
    const paperFolderPath = path.join(__dirname, '..', 'MyPapers');
    const folders = await fs.readdir(paperFolderPath);

    let foundFolder = null;
    for (const folder of folders) {
      if (folder.includes(`_${paperId}`)) {
        foundFolder = folder;
        break;
      }
    }

    if (!foundFolder) {
      return res.status(404).json({
        success: false,
        error: 'Dossier du paper non trouv√©'
      });
    }

    const chatHistoryPath = path.join(paperFolderPath, foundFolder, `${paperId}_aiChat.json`);

    // V√©rifier si le fichier existe
    if (!await fs.pathExists(chatHistoryPath)) {
      return res.json({
        success: true,
        data: {
          messages: [],
          exists: false
        }
      });
    }

    // Charger l'historique
    const history = await fs.readJson(chatHistoryPath);

    res.json({
      success: true,
      data: {
        messages: history.messages || [],
        lastUpdated: history.lastUpdated,
        exists: true
      }
    });

  } catch (error) {
    console.error('Error loading chat history:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors du chargement de l\'historique'
    });
  }
});

module.exports = router;