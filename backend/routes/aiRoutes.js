const express = require('express');
const { exec } = require('child_process');
const { promisify } = require('util');
const fs = require('fs-extra');
const path = require('path');
const pdf = require('pdf-parse');

const router = express.Router();
const execAsync = promisify(exec);

// V√©rifier le statut d'Ollama
router.get('/ollama/status', async (req, res) => {
  try {
    // V√©rifier si Ollama est install√© et en cours d'ex√©cution
    const response = await fetch('http://localhost:11434/api/tags');
    const data = await response.json();

    res.json({
      success: true,
      data: {
        installed: true,
        running: true,
        models: data.models || []
      }
    });
  } catch (error) {
    // V√©rifier si Ollama est install√© mais pas en cours d'ex√©cution
    try {
      await execAsync('ollama --version');
      res.json({
        success: true,
        data: {
          installed: true,
          running: false,
          models: []
        }
      });
    } catch (versionError) {
      res.json({
        success: true,
        data: {
          installed: false,
          running: false,
          models: []
        }
      });
    }
  }
});

// Obtenir la liste des mod√®les disponibles
router.get('/ollama/models/available', async (req, res) => {
  try {
    const availableModels = [
      {
        name: 'llama3.2:3b',
        size: '2.0GB',
        description: 'Mod√®le l√©ger, excellent pour commencer',
        recommended: true
      },
      {
        name: 'phi3:mini',
        size: '2.3GB',
        description: 'Tr√®s efficace pour Q&A, optimis√© Microsoft',
        recommended: true
      },
      {
        name: 'mistral:7b',
        size: '4.1GB',
        description: 'Mod√®le plus puissant, n√©cessite plus de VRAM',
        recommended: false
      },
      {
        name: 'llama3.2:1b',
        size: '1.3GB',
        description: 'Ultra-l√©ger, pour machines tr√®s limit√©es',
        recommended: false
      },
      {
        name: 'qwen2.5:7b',
        size: '4.7GB',
        description: 'Excellent pour l\'analyse de documents',
        recommended: false
      }
    ];

    res.json({
      success: true,
      data: availableModels
    });
  } catch (error) {
    console.error('Error getting available models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des mod√®les disponibles'
    });
  }
});

// Rechercher dans la biblioth√®que compl√®te des mod√®les Ollama
router.get('/ollama/models/search', async (req, res) => {
  try {
    const { q: query } = req.query;

    // Base de donn√©es √©tendue des mod√®les populaires Ollama
    const fullModelLibrary = [
      // LLaMA Family
      { name: 'llama3.2:1b', size: '1.3GB', description: 'Ultra-l√©ger, pour machines tr√®s limit√©es', category: 'LLaMA', params: '1B' },
      { name: 'llama3.2:3b', size: '2.0GB', description: 'Mod√®le l√©ger, excellent pour commencer', category: 'LLaMA', params: '3B' },
      { name: 'llama3.1:8b', size: '4.7GB', description: 'Mod√®le √©quilibr√© performance/taille', category: 'LLaMA', params: '8B' },
      { name: 'llama3.1:70b', size: '40GB', description: 'Mod√®le tr√®s puissant, n√©cessite beaucoup de VRAM', category: 'LLaMA', params: '70B' },

      // Mistral Family
      { name: 'mistral:7b', size: '4.1GB', description: 'Mod√®le puissant, excellent pour le fran√ßais', category: 'Mistral', params: '7B' },
      { name: 'mistral-nemo:12b', size: '7.1GB', description: 'Version am√©lior√©e de Mistral', category: 'Mistral', params: '12B' },
      { name: 'mixtral:8x7b', size: '26GB', description: 'Mod√®le mixture of experts tr√®s performant', category: 'Mistral', params: '47B' },

      // Phi Family (Microsoft)
      { name: 'phi3:mini', size: '2.3GB', description: 'Tr√®s efficace pour Q&A, optimis√© Microsoft', category: 'Phi', params: '3.8B' },
      { name: 'phi3:medium', size: '7.9GB', description: 'Version plus puissante de Phi-3', category: 'Phi', params: '14B' },

      // Qwen Family (Alibaba)
      { name: 'qwen2.5:0.5b', size: '0.4GB', description: 'Ultra-compact, id√©al pour tests', category: 'Qwen', params: '0.5B' },
      { name: 'qwen2.5:1.5b', size: '0.9GB', description: 'Tr√®s l√©ger mais performant', category: 'Qwen', params: '1.5B' },
      { name: 'qwen2.5:3b', size: '1.9GB', description: 'Bon √©quilibre taille/performance', category: 'Qwen', params: '3B' },
      { name: 'qwen2.5:7b', size: '4.7GB', description: 'Excellent pour l\'analyse de documents', category: 'Qwen', params: '7B' },
      { name: 'qwen2.5:14b', size: '8.2GB', description: 'Version plus puissante', category: 'Qwen', params: '14B' },
      { name: 'qwen2.5:32b', size: '18GB', description: 'Tr√®s performant pour les t√¢ches complexes', category: 'Qwen', params: '32B' },

      // Code Models
      { name: 'codellama:7b', size: '3.8GB', description: 'Sp√©cialis√© dans la g√©n√©ration de code', category: 'Code', params: '7B' },
      { name: 'codegemma:7b', size: '5.0GB', description: 'Mod√®le Google sp√©cialis√© code', category: 'Code', params: '7B' },
      { name: 'deepseek-coder:6.7b', size: '3.8GB', description: 'Excellent pour le code, tr√®s performant', category: 'Code', params: '6.7B' },
      { name: 'starcoder2:3b', size: '1.7GB', description: 'Mod√®le de code l√©ger et efficace', category: 'Code', params: '3B' },

      // Specialized Models
      { name: 'gemma2:2b', size: '1.6GB', description: 'Mod√®le Google compact et efficace', category: 'Gemma', params: '2B' },
      { name: 'gemma2:9b', size: '5.4GB', description: 'Version plus puissante de Gemma', category: 'Gemma', params: '9B' },
      { name: 'gemma2:27b', size: '16GB', description: 'Mod√®le Google haut de gamme', category: 'Gemma', params: '27B' },

      // Embedding Models
      { name: 'nomic-embed-text', size: '274MB', description: 'Mod√®le d\'embeddings pour la recherche s√©mantique', category: 'Embedding', params: '137M' },
      { name: 'mxbai-embed-large', size: '669MB', description: 'Embeddings haute qualit√©', category: 'Embedding', params: '335M' },

      // Vision Models
      { name: 'llava:7b', size: '4.7GB', description: 'Mod√®le multimodal (texte + images)', category: 'Vision', params: '7B' },
      { name: 'llava:13b', size: '8.0GB', description: 'Version plus puissante de LLaVA', category: 'Vision', params: '13B' },

      // Other Popular Models
      { name: 'neural-chat:7b', size: '4.1GB', description: 'Optimis√© pour les conversations', category: 'Chat', params: '7B' },
      { name: 'openchat:7b', size: '4.1GB', description: 'Mod√®le de chat tr√®s performant', category: 'Chat', params: '7B' },
      { name: 'wizard-vicuna-uncensored:7b', size: '4.1GB', description: 'Mod√®le sans censure pour cr√©ativit√©', category: 'Uncensored', params: '7B' },
      { name: 'orca2:7b', size: '4.1GB', description: 'Mod√®le Microsoft pour raisonnement', category: 'Reasoning', params: '7B' },
      { name: 'dolphin-mixtral:8x7b', size: '26GB', description: 'Version fine-tun√©e de Mixtral', category: 'Dolphin', params: '47B' },
    ];

    let results = fullModelLibrary;

    // Filtrer par query si fournie
    if (query && query.trim()) {
      const searchTerm = query.toLowerCase();
      results = fullModelLibrary.filter(model =>
        model.name.toLowerCase().includes(searchTerm) ||
        model.description.toLowerCase().includes(searchTerm) ||
        model.category.toLowerCase().includes(searchTerm)
      );
    }

    // Trier par popularit√©/recommandation (les plus petits d'abord dans chaque cat√©gorie)
    results.sort((a, b) => {
      // D'abord par cat√©gorie
      if (a.category !== b.category) {
        const categoryOrder = ['LLaMA', 'Mistral', 'Phi', 'Qwen', 'Gemma', 'Code', 'Chat', 'Vision', 'Embedding'];
        return categoryOrder.indexOf(a.category) - categoryOrder.indexOf(b.category);
      }
      // Puis par taille (plus petit d'abord)
      const sizeA = parseFloat(a.size);
      const sizeB = parseFloat(b.size);
      return sizeA - sizeB;
    });

    res.json({
      success: true,
      data: {
        models: results,
        total: results.length,
        query: query || ''
      }
    });
  } catch (error) {
    console.error('Error searching models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la recherche de mod√®les'
    });
  }
});

// Obtenir la liste des mod√®les install√©s
router.get('/ollama/models/installed', async (req, res) => {
  try {
    const response = await fetch('http://localhost:11434/api/tags');
    const data = await response.json();

    res.json({
      success: true,
      data: data.models || []
    });
  } catch (error) {
    console.error('Error getting installed models:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des mod√®les install√©s'
    });
  }
});

// T√©l√©charger un mod√®le
router.post('/ollama/models/pull', async (req, res) => {
  const { modelName } = req.body;

  if (!modelName) {
    return res.status(400).json({
      success: false,
      error: 'Le nom du mod√®le est requis'
    });
  }

  try {
    // D√©marrer le t√©l√©chargement en arri√®re-plan avec logging am√©lior√©
    console.log(`üöÄ D√©marrage du t√©l√©chargement de ${modelName}...`);

    const pullProcess = exec(`ollama pull ${modelName}`, (error, stdout, stderr) => {
      if (error) {
        console.error(`‚ùå Erreur lors du t√©l√©chargement de ${modelName}:`, error);
        return;
      }
      console.log(`‚úÖ Mod√®le ${modelName} t√©l√©charg√© avec succ√®s`);
      console.log('Sortie:', stdout);
    });

    // Log progress en temps r√©el
    pullProcess.stdout?.on('data', (data) => {
      console.log(`üì• ${modelName}: ${data.toString().trim()}`);
    });

    pullProcess.stderr?.on('data', (data) => {
      console.log(`üîÑ ${modelName}: ${data.toString().trim()}`);
    });

    res.json({
      success: true,
      message: `T√©l√©chargement du mod√®le ${modelName} d√©marr√©. V√©rifiez les logs du serveur pour le progr√®s.`,
      data: {
        modelName,
        tip: 'Le t√©l√©chargement peut prendre plusieurs minutes selon la taille du mod√®le. Utilisez le bouton "Actualiser" pour v√©rifier l\'installation.'
      }
    });
  } catch (error) {
    console.error('Error starting model pull:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors du d√©marrage du t√©l√©chargement'
    });
  }
});

// Supprimer un mod√®le
router.delete('/ollama/models/:modelName', async (req, res) => {
  const { modelName } = req.params;

  try {
    await execAsync(`ollama rm ${modelName}`);

    res.json({
      success: true,
      message: `Mod√®le ${modelName} supprim√© avec succ√®s`
    });
  } catch (error) {
    console.error(`Error removing model ${modelName}:`, error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la suppression du mod√®le'
    });
  }
});

// Tester la connexion avec un mod√®le
router.post('/ollama/test', async (req, res) => {
  const { modelName } = req.body;

  if (!modelName) {
    return res.status(400).json({
      success: false,
      error: 'Le nom du mod√®le est requis'
    });
  }

  try {
    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: modelName,
        prompt: 'R√©ponds simplement "Test r√©ussi" en fran√ßais.',
        stream: false
      })
    });

    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }

    const data = await response.json();

    res.json({
      success: true,
      message: 'Test de connexion r√©ussi',
      data: {
        modelName,
        response: data.response
      }
    });
  } catch (error) {
    console.error(`Error testing model ${modelName}:`, error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors du test de connexion'
    });
  }
});

// Obtenir les informations syst√®me (VRAM, etc.)
router.get('/system/info', async (req, res) => {
  try {
    // Utiliser les commandes syst√®me pour obtenir des infos sur la VRAM/RAM
    let systemInfo = {
      ram: 'N/A',
      gpu: 'N/A',
      storage: 'N/A'
    };

    try {
      // Tentative d'obtenir des infos RAM (Windows)
      const { stdout: ramInfo } = await execAsync('wmic computersystem get TotalPhysicalMemory /value');
      const ramMatch = ramInfo.match(/TotalPhysicalMemory=(\d+)/);
      if (ramMatch) {
        const ramBytes = parseInt(ramMatch[1]);
        const ramGB = Math.round(ramBytes / (1024 * 1024 * 1024));
        systemInfo.ram = `${ramGB} GB`;
      }
    } catch (error) {
      console.log('Could not get RAM info:', error.message);
    }

    try {
      // Tentative d'obtenir des infos GPU (Windows)
      const { stdout: gpuInfo } = await execAsync('wmic path win32_VideoController get name /value');
      const gpuMatch = gpuInfo.match(/Name=(.+)/);
      if (gpuMatch) {
        systemInfo.gpu = gpuMatch[1].trim();
      }
    } catch (error) {
      console.log('Could not get GPU info:', error.message);
    }

    res.json({
      success: true,
      data: systemInfo
    });
  } catch (error) {
    console.error('Error getting system info:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la r√©cup√©ration des informations syst√®me'
    });
  }
});

// Extraire le texte d'un PDF pour l'analyse IA
router.post('/pdf/extract', async (req, res) => {
  try {
    const { paperId } = req.body;

    if (!paperId) {
      return res.status(400).json({
        success: false,
        error: 'L\'ID du paper est requis'
      });
    }

    // Trouver le fichier PDF dans le dossier MyPapers
    const paperFolderPath = path.join(__dirname, '..', 'MyPapers');
    const folders = await fs.readdir(paperFolderPath);

    let pdfPath = null;
    let foundFolder = null;

    // Chercher le dossier contenant l'ID du paper
    for (const folder of folders) {
      if (folder.includes(`_${paperId}`)) {
        foundFolder = folder;
        const folderPath = path.join(paperFolderPath, folder);
        const files = await fs.readdir(folderPath);

        // Chercher le fichier PDF
        const pdfFile = files.find(file => file.toLowerCase().endsWith('.pdf'));
        if (pdfFile) {
          pdfPath = path.join(folderPath, pdfFile);
          break;
        }
      }
    }

    if (!pdfPath || !await fs.pathExists(pdfPath)) {
      return res.status(404).json({
        success: false,
        error: 'Fichier PDF non trouv√© pour cet article'
      });
    }

    // Extraire le texte du PDF
    const dataBuffer = await fs.readFile(pdfPath);
    const data = await pdf(dataBuffer);

    res.json({
      success: true,
      data: {
        text: data.text,
        pages: data.numpages,
        info: data.info,
        metadata: {
          paperId,
          folderPath: foundFolder,
          pdfPath: path.basename(pdfPath),
          extractedAt: new Date().toISOString()
        }
      }
    });

  } catch (error) {
    console.error('Error extracting PDF text:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de l\'extraction du texte PDF'
    });
  }
});

// Chat avec l'IA sur un document
router.post('/chat', async (req, res) => {
  try {
    const { message, context, modelName = 'llama3.1:8b' } = req.body;

    if (!message) {
      return res.status(400).json({
        success: false,
        error: 'Le message est requis'
      });
    }

    // Construire le prompt avec le contexte du document
    let fullPrompt = '';

    if (context && context.text) {
      fullPrompt = `Tu es un assistant IA sp√©cialis√© dans l'analyse d'articles de recherche scientifique.

CONTEXTE - Voici le contenu de l'article √† analyser :

${context.text.substring(0, 20000)} ${context.text.length > 20000 ? '...' : ''}

QUESTION DE L'UTILISATEUR :
${message}

INSTRUCTIONS :
- R√©ponds uniquement en fran√ßais
- Base ta r√©ponse sur le contenu de l'article fourni
- Sois pr√©cis et factuel
- Si l'information n'est pas dans l'article, dis-le clairement
- Structure ta r√©ponse de mani√®re claire et lisible

R√âPONSE :`;
    } else {
      fullPrompt = `Tu es un assistant IA pour l'analyse d'articles scientifiques. R√©ponds en fran√ßais √† cette question : ${message}`;
    }

    // Appeler Ollama
    const response = await fetch('http://localhost:11434/api/generate', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: modelName,
        prompt: fullPrompt,
        stream: false,
        options: {
          temperature: 0.1, // Plus d√©terministe pour l'analyse acad√©mique
          top_p: 0.9,
          top_k: 40
        }
      })
    });

    if (!response.ok) {
      throw new Error(`Ollama error: ${response.status}`);
    }

    const data = await response.json();

    res.json({
      success: true,
      data: {
        response: data.response,
        model: modelName,
        timestamp: new Date().toISOString(),
        tokenCount: data.eval_count || 0,
        processingTime: data.eval_duration ? Math.round(data.eval_duration / 1000000) : 0 // Convert to ms
      }
    });

  } catch (error) {
    console.error('Error in AI chat:', error);
    res.status(500).json({
      success: false,
      error: 'Erreur lors de la communication avec l\'IA'
    });
  }
});

module.exports = router;