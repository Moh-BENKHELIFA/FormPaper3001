# Installation et Configuration de PaperQA

Ce guide explique comment installer et configurer PaperQA (Mode 2: Ollama + Groq) dans FormPaper3001.

## üìã Pr√©requis

1. **Python 3.11+** install√©
2. **Ollama** install√© et en cours d'ex√©cution
3. **Cl√© API Groq** (gratuite sur https://console.groq.com)
4. **Node.js** et **npm** d√©j√† install√©s

## üöÄ Installation du Service PaperQA

### √âtape 1 : V√©rifier Python

```bash
python --version
# Doit afficher Python 3.11 ou sup√©rieur
```

Si Python n'est pas install√©, t√©l√©chargez-le depuis https://www.python.org/downloads/

### √âtape 2 : Installer les d√©pendances Python

```bash
cd paperqa-service

# Cr√©er un environnement virtuel
python -m venv venv

# Activer l'environnement (Windows)
venv\Scripts\activate

# Activer l'environnement (Linux/Mac)
source venv/bin/activate

# Installer les d√©pendances
pip install -r requirements.txt
```

**Note:** L'installation peut prendre 5-10 minutes (torch, transformers, etc.)

### √âtape 3 : Configurer la cl√© API Groq

```bash
# Cr√©er le fichier .env
cp .env.example .env
```

√âditer `.env` et ajouter votre cl√© Groq :
```
GROQ_API_KEY=gsk_votre_cl√©_ici
```

### √âtape 4 : V√©rifier qu'Ollama fonctionne

```bash
# Lancer Ollama (dans un terminal s√©par√©)
ollama serve

# V√©rifier qu'un mod√®le est install√©
ollama list

# Si llama3.1:8b n'est pas install√© :
ollama pull llama3.1:8b
```

### √âtape 5 : Lancer le service PaperQA

```bash
# Dans le dossier paperqa-service avec venv activ√©
python api.py
```

Vous devriez voir :
```
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### √âtape 6 : Tester le service

Dans un autre terminal :
```bash
curl http://localhost:8000/health
# Expected: {"status":"healthy","service":"paperqa"}
```

## üîÑ Lancer tous les services

### Terminal 1 : Service PaperQA
```bash
cd paperqa-service
venv\Scripts\activate  # Windows
python api.py
```

### Terminal 2 : Backend Node.js
```bash
cd backend
node server.js
```

### Terminal 3 : Frontend
```bash
cd frontend
npm run dev
```

## üß™ Test complet

1. Ouvrir le frontend : http://localhost:8668
2. Ouvrir un article
3. Dans le chat, cocher "PaperQA (Citations pr√©cises)"
4. Si l'article n'est pas index√©, l'indexation d√©marre automatiquement
5. Attendre l'indexation (5-10 min avec Ollama)
6. Poser une question
7. Recevoir une r√©ponse avec citations !

## üìä Architecture

```
Mode 2: Ollama (indexation) + Groq (requ√™tes)

Indexation (1 fois par PDF):
PDF ‚Üí PaperQA ‚Üí Ollama local ‚Üí Index cr√©√© (5-10 min)

Requ√™tes (chaque question):
Question ‚Üí PaperQA ‚Üí Groq API ‚Üí R√©ponse avec citations (5-10 sec)
```

## ‚öôÔ∏è Ports utilis√©s

- Frontend : 8668
- Backend Node.js : 5004
- Service PaperQA : 8000
- Ollama : 11434

## üêõ D√©pannage

### Erreur "PaperQA service not available"
- V√©rifier que le service Python tourne sur le port 8000
- V√©rifier les logs du service PaperQA

### Erreur lors de l'indexation
- V√©rifier qu'Ollama est lanc√© : `ollama serve`
- V√©rifier que llama3.1:8b est install√© : `ollama list`
- Regarder les logs du service PaperQA

### Erreur lors des requ√™tes
- V√©rifier que la cl√© Groq est correcte dans `.env`
- V√©rifier les limites de Groq (30 req/min)

### L'indexation est tr√®s lente
- C'est normal avec Ollama sans GPU
- Attendre 5-10 minutes pour un PDF de 20 pages
- Alternative : utiliser un mod√®le plus petit (llama3.1:8b ‚Üí llama3.2:3b)

## üí° Optimisations possibles

1. **Utiliser un GPU** pour acc√©l√©rer Ollama
2. **Cache l'index** : une fois index√©, pas besoin de r√©indexer
3. **Mod√®le plus petit** pour l'indexation : llama3.2:3b au lieu de llama3.1:8b

## üìö Documentation

- PaperQA: https://github.com/Future-House/paper-qa
- Ollama: https://ollama.ai
- Groq: https://console.groq.com
